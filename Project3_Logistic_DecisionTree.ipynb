{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "from datetime import datetime\n",
    "start_time = datetime.now()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**selecting necessary data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top 30 variables\n",
    "\n",
    "topvars = pd.read_csv('top_30_vars_final.csv')\n",
    "topvars = list(topvars.var_name.values)\n",
    "\n",
    "#all data\n",
    "data = pd.read_csv('card_transactions_all_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columnskeep = topvars + ['Recnum','Date','Fraud']\n",
    "len(columnskeep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data[columnskeep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cardnum_max_30</th>\n",
       "      <th>card_merchant_max_30</th>\n",
       "      <th>Merchnum_total_3</th>\n",
       "      <th>Cardnum_total_130</th>\n",
       "      <th>Merchnum_mean_7</th>\n",
       "      <th>card_merchant_median_1</th>\n",
       "      <th>Merchnum_mean_3</th>\n",
       "      <th>Cardnum_mean_30</th>\n",
       "      <th>card_merchant_mean_1</th>\n",
       "      <th>card_state_mean_30</th>\n",
       "      <th>card_state_median_30</th>\n",
       "      <th>card_zip_median_1</th>\n",
       "      <th>card_state_mean_14</th>\n",
       "      <th>Cardnum_max_14</th>\n",
       "      <th>card_state_max_0</th>\n",
       "      <th>Cardnum_total_30</th>\n",
       "      <th>card_zip_mean_1</th>\n",
       "      <th>card_state_median_14</th>\n",
       "      <th>card_zip_max_14</th>\n",
       "      <th>card_state_total_30</th>\n",
       "      <th>card_zip_mean_7</th>\n",
       "      <th>card_state_mean_7</th>\n",
       "      <th>card_state_median_7</th>\n",
       "      <th>card_zip_max_30</th>\n",
       "      <th>Merchnum_mean_1</th>\n",
       "      <th>card_merchant_mean_3</th>\n",
       "      <th>card_merchant_max_14</th>\n",
       "      <th>card_merchant_max_7</th>\n",
       "      <th>card_merchant_mean_7</th>\n",
       "      <th>card_zip_max_7</th>\n",
       "      <th>Recnum</th>\n",
       "      <th>Date</th>\n",
       "      <th>Fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.62</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.62</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.62</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.62</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>7.24</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>5</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cardnum_max_30  card_merchant_max_30  Merchnum_total_3  Cardnum_total_130  \\\n",
       "0            0.00                  0.00              0.00                NaN   \n",
       "1            0.00                  0.00              0.00                NaN   \n",
       "2            0.00                  0.00              0.00                NaN   \n",
       "3            0.00                  0.00              3.62                NaN   \n",
       "4            3.62                  3.62              7.24               30.0   \n",
       "\n",
       "   Merchnum_mean_7  card_merchant_median_1  Merchnum_mean_3  Cardnum_mean_30  \\\n",
       "0             0.00                    0.00             0.00             0.00   \n",
       "1             0.00                    0.00             0.00             0.00   \n",
       "2             0.00                    0.00             0.00             0.00   \n",
       "3             3.62                    0.00             3.62             0.00   \n",
       "4             3.62                    3.62             3.62             3.62   \n",
       "\n",
       "   card_merchant_mean_1  card_state_mean_30  card_state_median_30  \\\n",
       "0                  0.00                0.00                  0.00   \n",
       "1                  0.00                0.00                  0.00   \n",
       "2                  0.00                0.00                  0.00   \n",
       "3                  0.00                0.00                  0.00   \n",
       "4                  3.62                3.62                  3.62   \n",
       "\n",
       "   card_zip_median_1  card_state_mean_14  Cardnum_max_14  card_state_max_0  \\\n",
       "0               0.00                0.00            0.00              0.00   \n",
       "1               0.00                0.00            0.00              0.00   \n",
       "2               0.00                0.00            0.00              0.00   \n",
       "3               0.00                0.00            0.00              0.00   \n",
       "4               3.62                3.62            3.62              3.62   \n",
       "\n",
       "   Cardnum_total_30  card_zip_mean_1  card_state_median_14  card_zip_max_14  \\\n",
       "0              0.00             0.00                  0.00             0.00   \n",
       "1              0.00             0.00                  0.00             0.00   \n",
       "2              0.00             0.00                  0.00             0.00   \n",
       "3              0.00             0.00                  0.00             0.00   \n",
       "4              3.62             3.62                  3.62             3.62   \n",
       "\n",
       "   card_state_total_30  card_zip_mean_7  card_state_mean_7  \\\n",
       "0                 0.00             0.00               0.00   \n",
       "1                 0.00             0.00               0.00   \n",
       "2                 0.00             0.00               0.00   \n",
       "3                 0.00             0.00               0.00   \n",
       "4                 3.62             3.62               3.62   \n",
       "\n",
       "   card_state_median_7  card_zip_max_30  Merchnum_mean_1  \\\n",
       "0                 0.00             0.00             0.00   \n",
       "1                 0.00             0.00             0.00   \n",
       "2                 0.00             0.00             0.00   \n",
       "3                 0.00             0.00             3.62   \n",
       "4                 3.62             3.62             3.62   \n",
       "\n",
       "   card_merchant_mean_3  card_merchant_max_14  card_merchant_max_7  \\\n",
       "0                  0.00                  0.00                 0.00   \n",
       "1                  0.00                  0.00                 0.00   \n",
       "2                  0.00                  0.00                 0.00   \n",
       "3                  0.00                  0.00                 0.00   \n",
       "4                  3.62                  3.62                 3.62   \n",
       "\n",
       "   card_merchant_mean_7  card_zip_max_7  Recnum        Date  Fraud  \n",
       "0                  0.00            0.00       1  2010-01-01      0  \n",
       "1                  0.00            0.00       2  2010-01-01      0  \n",
       "2                  0.00            0.00       3  2010-01-01      0  \n",
       "3                  0.00            0.00       4  2010-01-01      0  \n",
       "4                  3.62            3.62       5  2010-01-01      0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**splitting modeling and oot data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_Xs = topvars\n",
    "\n",
    "#oot df\n",
    "df_oot = df[df.Date>='2010-11-01'].copy()\n",
    "\n",
    "#x and y oot\n",
    "y_oot = df_oot['Fraud'].copy()\n",
    "X_oot = df_oot.copy().drop(columns='Fraud')\n",
    "\n",
    "#modeling data\n",
    "df_model = df[(df.Date>'2010-01-14')&(df.Date<'2010-11-01')].copy()\n",
    "\n",
    "# #x and y train\n",
    "y = df_model['Fraud'].copy()\n",
    "X = df_model.copy().drop(columns='Fraud')\n",
    "\n",
    "# #dropping date and record\n",
    "df_oot.drop(columns=['Date','Recnum'], inplace=True)\n",
    "y_oot.drop(columns=['Date','Recnum'], inplace=True)\n",
    "X_oot.drop(columns=['Date','Recnum'], inplace=True)\n",
    "df_model.drop(columns=['Date','Recnum'], inplace=True)\n",
    "y.drop(columns=['Date','Recnum'], inplace=True)\n",
    "X.drop(columns=['Date','Recnum'], inplace=True)\n",
    "\n",
    "# X_oot = StandardScaler().fit_transform(X_oot)\n",
    "# X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cardnum_max_30</th>\n",
       "      <th>card_merchant_max_30</th>\n",
       "      <th>Merchnum_total_3</th>\n",
       "      <th>Cardnum_total_130</th>\n",
       "      <th>Merchnum_mean_7</th>\n",
       "      <th>card_merchant_median_1</th>\n",
       "      <th>Merchnum_mean_3</th>\n",
       "      <th>Cardnum_mean_30</th>\n",
       "      <th>card_merchant_mean_1</th>\n",
       "      <th>card_state_mean_30</th>\n",
       "      <th>card_state_median_30</th>\n",
       "      <th>card_zip_median_1</th>\n",
       "      <th>card_state_mean_14</th>\n",
       "      <th>Cardnum_max_14</th>\n",
       "      <th>card_state_max_0</th>\n",
       "      <th>Cardnum_total_30</th>\n",
       "      <th>card_zip_mean_1</th>\n",
       "      <th>card_state_median_14</th>\n",
       "      <th>card_zip_max_14</th>\n",
       "      <th>card_state_total_30</th>\n",
       "      <th>card_zip_mean_7</th>\n",
       "      <th>card_state_mean_7</th>\n",
       "      <th>card_state_median_7</th>\n",
       "      <th>card_zip_max_30</th>\n",
       "      <th>Merchnum_mean_1</th>\n",
       "      <th>card_merchant_mean_3</th>\n",
       "      <th>card_merchant_max_14</th>\n",
       "      <th>card_merchant_max_7</th>\n",
       "      <th>card_merchant_mean_7</th>\n",
       "      <th>card_zip_max_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3338</th>\n",
       "      <td>1593.38</td>\n",
       "      <td>983.66</td>\n",
       "      <td>520.59</td>\n",
       "      <td>4.530289</td>\n",
       "      <td>291.155000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>130.147500</td>\n",
       "      <td>403.136500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>813.400000</td>\n",
       "      <td>813.40</td>\n",
       "      <td>0.000</td>\n",
       "      <td>813.400000</td>\n",
       "      <td>1593.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8062.73</td>\n",
       "      <td>0.000</td>\n",
       "      <td>813.40</td>\n",
       "      <td>983.66</td>\n",
       "      <td>1626.80</td>\n",
       "      <td>813.400000</td>\n",
       "      <td>813.400000</td>\n",
       "      <td>813.40</td>\n",
       "      <td>983.66</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>983.66</td>\n",
       "      <td>983.66</td>\n",
       "      <td>813.400000</td>\n",
       "      <td>983.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3339</th>\n",
       "      <td>656.50</td>\n",
       "      <td>15.40</td>\n",
       "      <td>827.47</td>\n",
       "      <td>16.660322</td>\n",
       "      <td>6.272880</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.522455</td>\n",
       "      <td>10.018220</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.492735</td>\n",
       "      <td>3.80</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.492735</td>\n",
       "      <td>656.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1182.15</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.80</td>\n",
       "      <td>15.40</td>\n",
       "      <td>525.65</td>\n",
       "      <td>4.812286</td>\n",
       "      <td>4.812286</td>\n",
       "      <td>3.80</td>\n",
       "      <td>15.40</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>5.194839</td>\n",
       "      <td>15.40</td>\n",
       "      <td>15.40</td>\n",
       "      <td>4.706765</td>\n",
       "      <td>15.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3340</th>\n",
       "      <td>2472.00</td>\n",
       "      <td>554.38</td>\n",
       "      <td>311.01</td>\n",
       "      <td>5.433914</td>\n",
       "      <td>155.505000</td>\n",
       "      <td>155.505</td>\n",
       "      <td>155.505000</td>\n",
       "      <td>454.118438</td>\n",
       "      <td>155.505</td>\n",
       "      <td>267.456667</td>\n",
       "      <td>302.99</td>\n",
       "      <td>155.505</td>\n",
       "      <td>267.456667</td>\n",
       "      <td>2472.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14531.79</td>\n",
       "      <td>155.505</td>\n",
       "      <td>302.99</td>\n",
       "      <td>554.38</td>\n",
       "      <td>2407.11</td>\n",
       "      <td>155.505000</td>\n",
       "      <td>227.690000</td>\n",
       "      <td>278.73</td>\n",
       "      <td>554.38</td>\n",
       "      <td>155.505000</td>\n",
       "      <td>155.505000</td>\n",
       "      <td>554.38</td>\n",
       "      <td>278.73</td>\n",
       "      <td>155.505000</td>\n",
       "      <td>278.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3341</th>\n",
       "      <td>656.50</td>\n",
       "      <td>15.40</td>\n",
       "      <td>831.84</td>\n",
       "      <td>16.709453</td>\n",
       "      <td>6.262969</td>\n",
       "      <td>4.370</td>\n",
       "      <td>7.494054</td>\n",
       "      <td>9.970756</td>\n",
       "      <td>4.370</td>\n",
       "      <td>4.491695</td>\n",
       "      <td>3.80</td>\n",
       "      <td>4.370</td>\n",
       "      <td>4.491695</td>\n",
       "      <td>656.50</td>\n",
       "      <td>4.37</td>\n",
       "      <td>1186.52</td>\n",
       "      <td>4.370</td>\n",
       "      <td>3.80</td>\n",
       "      <td>15.40</td>\n",
       "      <td>530.02</td>\n",
       "      <td>4.806056</td>\n",
       "      <td>4.806056</td>\n",
       "      <td>3.80</td>\n",
       "      <td>15.40</td>\n",
       "      <td>47.685000</td>\n",
       "      <td>5.169062</td>\n",
       "      <td>15.40</td>\n",
       "      <td>15.40</td>\n",
       "      <td>4.701884</td>\n",
       "      <td>15.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3342</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>113.87</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>241.670000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>113.870000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83965</th>\n",
       "      <td>929.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>606.55</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>288.095000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>151.637500</td>\n",
       "      <td>355.532500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1422.13</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>419.120000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83966</th>\n",
       "      <td>179.98</td>\n",
       "      <td>179.98</td>\n",
       "      <td>595.77</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>119.154000</td>\n",
       "      <td>179.980</td>\n",
       "      <td>119.154000</td>\n",
       "      <td>179.980000</td>\n",
       "      <td>179.980</td>\n",
       "      <td>179.980000</td>\n",
       "      <td>179.98</td>\n",
       "      <td>179.980</td>\n",
       "      <td>179.980000</td>\n",
       "      <td>179.98</td>\n",
       "      <td>179.98</td>\n",
       "      <td>179.98</td>\n",
       "      <td>179.980</td>\n",
       "      <td>179.98</td>\n",
       "      <td>179.98</td>\n",
       "      <td>179.98</td>\n",
       "      <td>179.980000</td>\n",
       "      <td>179.980000</td>\n",
       "      <td>179.98</td>\n",
       "      <td>179.98</td>\n",
       "      <td>159.143333</td>\n",
       "      <td>179.980000</td>\n",
       "      <td>179.98</td>\n",
       "      <td>179.98</td>\n",
       "      <td>179.980000</td>\n",
       "      <td>179.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83967</th>\n",
       "      <td>399.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>217.116667</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>399.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>651.35</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83968</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83969</th>\n",
       "      <td>365.32</td>\n",
       "      <td>365.32</td>\n",
       "      <td>906.32</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>289.762857</td>\n",
       "      <td>0.000</td>\n",
       "      <td>181.264000</td>\n",
       "      <td>365.320000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>365.320000</td>\n",
       "      <td>365.32</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>365.32</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>365.32</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>365.32</td>\n",
       "      <td>359.445000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80632 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Cardnum_max_30  card_merchant_max_30  Merchnum_total_3  \\\n",
       "3338          1593.38                983.66            520.59   \n",
       "3339           656.50                 15.40            827.47   \n",
       "3340          2472.00                554.38            311.01   \n",
       "3341           656.50                 15.40            831.84   \n",
       "3342             0.00                  0.00            113.87   \n",
       "...               ...                   ...               ...   \n",
       "83965          929.00                  0.00            606.55   \n",
       "83966          179.98                179.98            595.77   \n",
       "83967          399.15                  0.00              0.00   \n",
       "83968            0.00                  0.00              0.00   \n",
       "83969          365.32                365.32            906.32   \n",
       "\n",
       "       Cardnum_total_130  Merchnum_mean_7  card_merchant_median_1  \\\n",
       "3338            4.530289       291.155000                   0.000   \n",
       "3339           16.660322         6.272880                   0.000   \n",
       "3340            5.433914       155.505000                 155.505   \n",
       "3341           16.709453         6.262969                   4.370   \n",
       "3342            0.000000       241.670000                   0.000   \n",
       "...                  ...              ...                     ...   \n",
       "83965           0.000000       288.095000                   0.000   \n",
       "83966          30.000000       119.154000                 179.980   \n",
       "83967           0.000000         0.000000                   0.000   \n",
       "83968           0.000000         0.000000                   0.000   \n",
       "83969           0.000000       289.762857                   0.000   \n",
       "\n",
       "       Merchnum_mean_3  Cardnum_mean_30  card_merchant_mean_1  \\\n",
       "3338        130.147500       403.136500                 0.000   \n",
       "3339          7.522455        10.018220                 0.000   \n",
       "3340        155.505000       454.118438               155.505   \n",
       "3341          7.494054         9.970756                 4.370   \n",
       "3342        113.870000         0.000000                 0.000   \n",
       "...                ...              ...                   ...   \n",
       "83965       151.637500       355.532500                 0.000   \n",
       "83966       119.154000       179.980000               179.980   \n",
       "83967         0.000000       217.116667                 0.000   \n",
       "83968         0.000000         0.000000                 0.000   \n",
       "83969       181.264000       365.320000                 0.000   \n",
       "\n",
       "       card_state_mean_30  card_state_median_30  card_zip_median_1  \\\n",
       "3338           813.400000                813.40              0.000   \n",
       "3339             4.492735                  3.80              0.000   \n",
       "3340           267.456667                302.99            155.505   \n",
       "3341             4.491695                  3.80              4.370   \n",
       "3342             0.000000                  0.00              0.000   \n",
       "...                   ...                   ...                ...   \n",
       "83965            0.000000                  0.00              0.000   \n",
       "83966          179.980000                179.98            179.980   \n",
       "83967            0.000000                  0.00              0.000   \n",
       "83968            0.000000                  0.00              0.000   \n",
       "83969          365.320000                365.32              0.000   \n",
       "\n",
       "       card_state_mean_14  Cardnum_max_14  card_state_max_0  Cardnum_total_30  \\\n",
       "3338           813.400000         1593.38              0.00           8062.73   \n",
       "3339             4.492735          656.50              0.00           1182.15   \n",
       "3340           267.456667         2472.00              0.00          14531.79   \n",
       "3341             4.491695          656.50              4.37           1186.52   \n",
       "3342             0.000000            0.00              0.00              0.00   \n",
       "...                   ...             ...               ...               ...   \n",
       "83965            0.000000            0.00              0.00           1422.13   \n",
       "83966          179.980000          179.98            179.98            179.98   \n",
       "83967            0.000000          399.15              0.00            651.35   \n",
       "83968            0.000000            0.00              0.00              0.00   \n",
       "83969            0.000000            0.00              0.00            365.32   \n",
       "\n",
       "       card_zip_mean_1  card_state_median_14  card_zip_max_14  \\\n",
       "3338             0.000                813.40           983.66   \n",
       "3339             0.000                  3.80            15.40   \n",
       "3340           155.505                302.99           554.38   \n",
       "3341             4.370                  3.80            15.40   \n",
       "3342             0.000                  0.00             0.00   \n",
       "...                ...                   ...              ...   \n",
       "83965            0.000                  0.00             0.00   \n",
       "83966          179.980                179.98           179.98   \n",
       "83967            0.000                  0.00             0.00   \n",
       "83968            0.000                  0.00             0.00   \n",
       "83969            0.000                  0.00             0.00   \n",
       "\n",
       "       card_state_total_30  card_zip_mean_7  card_state_mean_7  \\\n",
       "3338               1626.80       813.400000         813.400000   \n",
       "3339                525.65         4.812286           4.812286   \n",
       "3340               2407.11       155.505000         227.690000   \n",
       "3341                530.02         4.806056           4.806056   \n",
       "3342                  0.00         0.000000           0.000000   \n",
       "...                    ...              ...                ...   \n",
       "83965                 0.00         0.000000           0.000000   \n",
       "83966               179.98       179.980000         179.980000   \n",
       "83967                 0.00         0.000000           0.000000   \n",
       "83968                 0.00         0.000000           0.000000   \n",
       "83969               365.32         0.000000           0.000000   \n",
       "\n",
       "       card_state_median_7  card_zip_max_30  Merchnum_mean_1  \\\n",
       "3338                813.40           983.66         0.000000   \n",
       "3339                  3.80            15.40        91.000000   \n",
       "3340                278.73           554.38       155.505000   \n",
       "3341                  3.80            15.40        47.685000   \n",
       "3342                  0.00             0.00         0.000000   \n",
       "...                    ...              ...              ...   \n",
       "83965                 0.00             0.00       419.120000   \n",
       "83966               179.98           179.98       159.143333   \n",
       "83967                 0.00             0.00         0.000000   \n",
       "83968                 0.00             0.00         0.000000   \n",
       "83969                 0.00           365.32       359.445000   \n",
       "\n",
       "       card_merchant_mean_3  card_merchant_max_14  card_merchant_max_7  \\\n",
       "3338               0.000000                983.66               983.66   \n",
       "3339               5.194839                 15.40                15.40   \n",
       "3340             155.505000                554.38               278.73   \n",
       "3341               5.169062                 15.40                15.40   \n",
       "3342               0.000000                  0.00                 0.00   \n",
       "...                     ...                   ...                  ...   \n",
       "83965              0.000000                  0.00                 0.00   \n",
       "83966            179.980000                179.98               179.98   \n",
       "83967              0.000000                  0.00                 0.00   \n",
       "83968              0.000000                  0.00                 0.00   \n",
       "83969              0.000000                  0.00                 0.00   \n",
       "\n",
       "       card_merchant_mean_7  card_zip_max_7  \n",
       "3338             813.400000          983.66  \n",
       "3339               4.706765           15.40  \n",
       "3340             155.505000          278.73  \n",
       "3341               4.701884           15.40  \n",
       "3342               0.000000            0.00  \n",
       "...                     ...             ...  \n",
       "83965              0.000000            0.00  \n",
       "83966            179.980000          179.98  \n",
       "83967              0.000000            0.00  \n",
       "83968              0.000000            0.00  \n",
       "83969              0.000000            0.00  \n",
       "\n",
       "[80632 rows x 30 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#caclulate fdr at 3%\n",
    "\n",
    "def calculateFDR(predict, fraudscore, y):\n",
    "    temp = pd.DataFrame({'true':y.tolist(), 'predict':predict, 'score':fraudscore})\n",
    "    temp.sort_values('score', ascending=False, inplace=True)\n",
    "    percent3 = int(len(predict)*0.03)\n",
    "    return np.sum(temp.true[0:percent3])/np.sum(temp.true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticregressorFDR(X, X_oot, y, y_oot, C = 1, iteration=10):\n",
    "    lr = LogisticRegression(C = C)\n",
    "    \n",
    "    fdr_df=[]\n",
    "    fdr_total=[0,0,0] # train, test, oot\n",
    "    for i in range(iteration):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y) # using default setting for test_size(0.25), random_state(None) and shuffle(True)\n",
    "        lr.fit(X_train, y_train)\n",
    "        \n",
    "        data = [[X_train, y_train], [X_test, y_test], [X_oot, y_oot]]\n",
    "        fdr_list=[]\n",
    "        for d in range(len(data)):\n",
    "            predict = lr.predict(data[d][0])\n",
    "            prob = lr.predict_proba(data[d][0])\n",
    "            fraudscore = prob.transpose()[1]\n",
    "            fdr_each = calculateFDR(predict, fraudscore, data[d][1])\n",
    "            fdr_total[d] += fdr_each\n",
    "            fdr_list.append(fdr_each)\n",
    "        \n",
    "        fdr_df.append(fdr_list)\n",
    "    \n",
    "    avg_train = fdr_total[0] / iteration\n",
    "    avg_test = fdr_total[1] / iteration\n",
    "    avg_oot = fdr_total[2] / iteration\n",
    "    result = [avg_train, avg_test, avg_oot]\n",
    "\n",
    "    return fdr_df, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_estimators: 5, with lr_fdr: 0.5205669971216349 on the test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/romansielewicz/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Adjusting hyperparameters\n",
    "features = [5, 10, 15, 20, 25, 30] # should be less than number of variables (original choice: [5,10,15,20])\n",
    "\n",
    "maximum = 0\n",
    "params=[]\n",
    "lr_fdr_all=[]\n",
    "lr_fdr_mean=[]\n",
    "for i in features:\n",
    "    # defining set of variables\n",
    "    X_temp = X.iloc[:, 0:i]\n",
    "    X_oot_temp = X_oot.iloc[:, 0:i]\n",
    "    # defining temp variables to store metrics\n",
    "    temp1, temp2 = logisticregressorFDR(X_temp, X_oot_temp, y, y_oot, \\\n",
    "                                        C = 0.01) # optimizing on test data\n",
    "    params.append(i)\n",
    "    lr_fdr_mean.append(temp2)\n",
    "    lr_fdr_all.append(temp1)\n",
    "    if maximum < temp2[1]:\n",
    "        maximum=temp2[1]\n",
    "        maxlr_fdr=temp2\n",
    "        maxparams=i\n",
    "                \n",
    "print(f'max_estimators: {maxparams}, with lr_fdr: {maximum} on the test data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>fdr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>[0.5051551892325348, 0.5205669971216349, 0.308...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>[0.09985481409596135, 0.10083655283834463, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>[0.12732521967663443, 0.13162672671204165, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>[0.14611251990428883, 0.1387079680430957, 0.10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>[0.15624991023471996, 0.15040139700413982, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30</td>\n",
       "      <td>[0.17792411430534755, 0.16852085957706348, 0.1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   params                                                fdr\n",
       "0       5  [0.5051551892325348, 0.5205669971216349, 0.308...\n",
       "1      10  [0.09985481409596135, 0.10083655283834463, 0.0...\n",
       "2      15  [0.12732521967663443, 0.13162672671204165, 0.0...\n",
       "3      20  [0.14611251990428883, 0.1387079680430957, 0.10...\n",
       "4      25  [0.15624991023471996, 0.15040139700413982, 0.1...\n",
       "5      30  [0.17792411430534755, 0.16852085957706348, 0.1..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_output30 = pd.DataFrame({'params':params, 'fdr':lr_fdr_mean})\n",
    "lr_output30_alliter = pd.DataFrame(lr_fdr_all[0],columns=[str(params[0])+': train', 'test', 'oot'])\n",
    "for i in range(1,len(lr_fdr_all)):\n",
    "    lr_output30_alliter = pd.concat([lr_output30_alliter, pd.DataFrame(lr_fdr_all[i],columns=[str(params[i])+': train', 'test', 'oot'])], axis=1)\n",
    "lr_output30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>5: train</th>\n",
       "      <th>test</th>\n",
       "      <th>oot</th>\n",
       "      <th>10: train</th>\n",
       "      <th>test</th>\n",
       "      <th>oot</th>\n",
       "      <th>15: train</th>\n",
       "      <th>test</th>\n",
       "      <th>oot</th>\n",
       "      <th>20: train</th>\n",
       "      <th>test</th>\n",
       "      <th>oot</th>\n",
       "      <th>25: train</th>\n",
       "      <th>test</th>\n",
       "      <th>oot</th>\n",
       "      <th>30: train</th>\n",
       "      <th>test</th>\n",
       "      <th>oot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.510511</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.307263</td>\n",
       "      <td>0.094136</td>\n",
       "      <td>0.059091</td>\n",
       "      <td>0.022346</td>\n",
       "      <td>0.138088</td>\n",
       "      <td>0.153110</td>\n",
       "      <td>0.122905</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.144231</td>\n",
       "      <td>0.117318</td>\n",
       "      <td>0.172840</td>\n",
       "      <td>0.159091</td>\n",
       "      <td>0.117318</td>\n",
       "      <td>0.174114</td>\n",
       "      <td>0.182648</td>\n",
       "      <td>0.117318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.521605</td>\n",
       "      <td>0.459091</td>\n",
       "      <td>0.307263</td>\n",
       "      <td>0.099248</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.039106</td>\n",
       "      <td>0.129969</td>\n",
       "      <td>0.112150</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.136646</td>\n",
       "      <td>0.138393</td>\n",
       "      <td>0.106145</td>\n",
       "      <td>0.168182</td>\n",
       "      <td>0.158654</td>\n",
       "      <td>0.122905</td>\n",
       "      <td>0.180937</td>\n",
       "      <td>0.140562</td>\n",
       "      <td>0.111732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.516381</td>\n",
       "      <td>0.493392</td>\n",
       "      <td>0.307263</td>\n",
       "      <td>0.070229</td>\n",
       "      <td>0.098592</td>\n",
       "      <td>0.016760</td>\n",
       "      <td>0.111773</td>\n",
       "      <td>0.137056</td>\n",
       "      <td>0.111732</td>\n",
       "      <td>0.153963</td>\n",
       "      <td>0.165094</td>\n",
       "      <td>0.117318</td>\n",
       "      <td>0.155015</td>\n",
       "      <td>0.128571</td>\n",
       "      <td>0.100559</td>\n",
       "      <td>0.173516</td>\n",
       "      <td>0.137441</td>\n",
       "      <td>0.122905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.500766</td>\n",
       "      <td>0.534884</td>\n",
       "      <td>0.307263</td>\n",
       "      <td>0.116564</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.055866</td>\n",
       "      <td>0.125382</td>\n",
       "      <td>0.158879</td>\n",
       "      <td>0.061453</td>\n",
       "      <td>0.131783</td>\n",
       "      <td>0.147982</td>\n",
       "      <td>0.106145</td>\n",
       "      <td>0.161742</td>\n",
       "      <td>0.146667</td>\n",
       "      <td>0.100559</td>\n",
       "      <td>0.169954</td>\n",
       "      <td>0.162679</td>\n",
       "      <td>0.117318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.484163</td>\n",
       "      <td>0.575610</td>\n",
       "      <td>0.307263</td>\n",
       "      <td>0.102761</td>\n",
       "      <td>0.134259</td>\n",
       "      <td>0.061453</td>\n",
       "      <td>0.140030</td>\n",
       "      <td>0.123223</td>\n",
       "      <td>0.111732</td>\n",
       "      <td>0.161787</td>\n",
       "      <td>0.100457</td>\n",
       "      <td>0.106145</td>\n",
       "      <td>0.161994</td>\n",
       "      <td>0.163717</td>\n",
       "      <td>0.139665</td>\n",
       "      <td>0.175079</td>\n",
       "      <td>0.196581</td>\n",
       "      <td>0.139665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.504532</td>\n",
       "      <td>0.548544</td>\n",
       "      <td>0.312849</td>\n",
       "      <td>0.124251</td>\n",
       "      <td>0.115000</td>\n",
       "      <td>0.078212</td>\n",
       "      <td>0.108453</td>\n",
       "      <td>0.161826</td>\n",
       "      <td>0.067039</td>\n",
       "      <td>0.157504</td>\n",
       "      <td>0.117949</td>\n",
       "      <td>0.117318</td>\n",
       "      <td>0.140600</td>\n",
       "      <td>0.161702</td>\n",
       "      <td>0.106145</td>\n",
       "      <td>0.201238</td>\n",
       "      <td>0.148649</td>\n",
       "      <td>0.128492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.503817</td>\n",
       "      <td>0.521127</td>\n",
       "      <td>0.312849</td>\n",
       "      <td>0.102009</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.027933</td>\n",
       "      <td>0.127132</td>\n",
       "      <td>0.116592</td>\n",
       "      <td>0.061453</td>\n",
       "      <td>0.138973</td>\n",
       "      <td>0.169903</td>\n",
       "      <td>0.083799</td>\n",
       "      <td>0.146154</td>\n",
       "      <td>0.155963</td>\n",
       "      <td>0.117318</td>\n",
       "      <td>0.176752</td>\n",
       "      <td>0.179167</td>\n",
       "      <td>0.128492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.503759</td>\n",
       "      <td>0.497537</td>\n",
       "      <td>0.307263</td>\n",
       "      <td>0.090484</td>\n",
       "      <td>0.074890</td>\n",
       "      <td>0.027933</td>\n",
       "      <td>0.136434</td>\n",
       "      <td>0.116592</td>\n",
       "      <td>0.083799</td>\n",
       "      <td>0.148260</td>\n",
       "      <td>0.140097</td>\n",
       "      <td>0.134078</td>\n",
       "      <td>0.149691</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.117318</td>\n",
       "      <td>0.185583</td>\n",
       "      <td>0.189815</td>\n",
       "      <td>0.122905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.506787</td>\n",
       "      <td>0.531707</td>\n",
       "      <td>0.307263</td>\n",
       "      <td>0.080670</td>\n",
       "      <td>0.080569</td>\n",
       "      <td>0.061453</td>\n",
       "      <td>0.118644</td>\n",
       "      <td>0.105023</td>\n",
       "      <td>0.117318</td>\n",
       "      <td>0.142209</td>\n",
       "      <td>0.120773</td>\n",
       "      <td>0.083799</td>\n",
       "      <td>0.150794</td>\n",
       "      <td>0.147059</td>\n",
       "      <td>0.106145</td>\n",
       "      <td>0.168224</td>\n",
       "      <td>0.185841</td>\n",
       "      <td>0.111732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.499232</td>\n",
       "      <td>0.543779</td>\n",
       "      <td>0.307263</td>\n",
       "      <td>0.118196</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.072626</td>\n",
       "      <td>0.137346</td>\n",
       "      <td>0.131818</td>\n",
       "      <td>0.100559</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.142202</td>\n",
       "      <td>0.117318</td>\n",
       "      <td>0.155488</td>\n",
       "      <td>0.146226</td>\n",
       "      <td>0.083799</td>\n",
       "      <td>0.173844</td>\n",
       "      <td>0.161826</td>\n",
       "      <td>0.122905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   5: train      test       oot  10: train      test       oot  15: train  \\\n",
       "0  0.510511  0.500000  0.307263   0.094136  0.059091  0.022346   0.138088   \n",
       "1  0.521605  0.459091  0.307263   0.099248  0.137931  0.039106   0.129969   \n",
       "2  0.516381  0.493392  0.307263   0.070229  0.098592  0.016760   0.111773   \n",
       "3  0.500766  0.534884  0.307263   0.116564  0.111111  0.055866   0.125382   \n",
       "4  0.484163  0.575610  0.307263   0.102761  0.134259  0.061453   0.140030   \n",
       "5  0.504532  0.548544  0.312849   0.124251  0.115000  0.078212   0.108453   \n",
       "6  0.503817  0.521127  0.312849   0.102009  0.076923  0.027933   0.127132   \n",
       "7  0.503759  0.497537  0.307263   0.090484  0.074890  0.027933   0.136434   \n",
       "8  0.506787  0.531707  0.307263   0.080670  0.080569  0.061453   0.118644   \n",
       "9  0.499232  0.543779  0.307263   0.118196  0.120000  0.072626   0.137346   \n",
       "\n",
       "       test       oot  20: train      test       oot  25: train      test  \\\n",
       "0  0.153110  0.122905   0.150000  0.144231  0.117318   0.172840  0.159091   \n",
       "1  0.112150  0.094972   0.136646  0.138393  0.106145   0.168182  0.158654   \n",
       "2  0.137056  0.111732   0.153963  0.165094  0.117318   0.155015  0.128571   \n",
       "3  0.158879  0.061453   0.131783  0.147982  0.106145   0.161742  0.146667   \n",
       "4  0.123223  0.111732   0.161787  0.100457  0.106145   0.161994  0.163717   \n",
       "5  0.161826  0.067039   0.157504  0.117949  0.117318   0.140600  0.161702   \n",
       "6  0.116592  0.061453   0.138973  0.169903  0.083799   0.146154  0.155963   \n",
       "7  0.116592  0.083799   0.148260  0.140097  0.134078   0.149691  0.136364   \n",
       "8  0.105023  0.117318   0.142209  0.120773  0.083799   0.150794  0.147059   \n",
       "9  0.131818  0.100559   0.140000  0.142202  0.117318   0.155488  0.146226   \n",
       "\n",
       "        oot  30: train      test       oot  \n",
       "0  0.117318   0.174114  0.182648  0.117318  \n",
       "1  0.122905   0.180937  0.140562  0.111732  \n",
       "2  0.100559   0.173516  0.137441  0.122905  \n",
       "3  0.100559   0.169954  0.162679  0.117318  \n",
       "4  0.139665   0.175079  0.196581  0.139665  \n",
       "5  0.106145   0.201238  0.148649  0.128492  \n",
       "6  0.117318   0.176752  0.179167  0.128492  \n",
       "7  0.117318   0.185583  0.189815  0.122905  \n",
       "8  0.106145   0.168224  0.185841  0.111732  \n",
       "9  0.083799   0.173844  0.161826  0.122905  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_output30_alliter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting to csv\n",
    "lr_output30.to_csv('Results/lr_30vars_mean.csv')\n",
    "lr_output30_alliter.to_csv('Results/lr_30vars_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of Variables</th>\n",
       "      <th>blank1</th>\n",
       "      <th>blank2</th>\n",
       "      <th>blank3</th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "      <th>OOT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.505155</td>\n",
       "      <td>0.520567</td>\n",
       "      <td>0.308380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.099855</td>\n",
       "      <td>0.100837</td>\n",
       "      <td>0.046369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.127325</td>\n",
       "      <td>0.131627</td>\n",
       "      <td>0.093296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.146113</td>\n",
       "      <td>0.138708</td>\n",
       "      <td>0.108939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.150401</td>\n",
       "      <td>0.111173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.177924</td>\n",
       "      <td>0.168521</td>\n",
       "      <td>0.122346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number of Variables  blank1  blank2  blank3     Train      Test       OOT\n",
       "0                    5     NaN     NaN     NaN  0.505155  0.520567  0.308380\n",
       "1                   10     NaN     NaN     NaN  0.099855  0.100837  0.046369\n",
       "2                   15     NaN     NaN     NaN  0.127325  0.131627  0.093296\n",
       "3                   20     NaN     NaN     NaN  0.146113  0.138708  0.108939\n",
       "4                   25     NaN     NaN     NaN  0.156250  0.150401  0.111173\n",
       "5                   30     NaN     NaN     NaN  0.177924  0.168521  0.122346"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating Summary Table\n",
    "lr_summary = lr_output30.copy()\n",
    "lr_summary['Number of Variables'] = lr_summary['params']\n",
    "lr_summary['blank1'] = np.nan\n",
    "lr_summary['blank2'] = np.nan\n",
    "lr_summary['blank3'] = np.nan\n",
    "lr_summary['Train'] = lr_summary['fdr'].apply(lambda x: x[0])\n",
    "lr_summary['Test'] = lr_summary['fdr'].apply(lambda x: x[1])\n",
    "lr_summary['OOT'] = lr_summary['fdr'].apply(lambda x: x[2])\n",
    "lr_summary.drop(columns = ['params', 'fdr'], inplace = True)\n",
    "lr_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_summary.to_excel('Results/lr_summary.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decisiontreeFDR(X, X_oot, y, y_oot, max_features, max_leaf_nodes, max_depth, iteration=10):\n",
    "    dt = DecisionTreeClassifier(max_features = max_features, \\\n",
    "                                    max_leaf_nodes = max_leaf_nodes, \\\n",
    "                                    max_depth = max_depth, \\\n",
    "                                    random_state=0)\n",
    "    \n",
    "    fdr_df=[]\n",
    "    fdr_total=[0,0,0] # train, test, oot\n",
    "    for i in range(iteration):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y) # using default setting for test_size(0.25), random_state(None) and shuffle(True)\n",
    "        dt.fit(X_train, y_train)\n",
    "        \n",
    "        data = [[X_train, y_train], [X_test, y_test], [X_oot, y_oot]]\n",
    "        fdr_list=[]\n",
    "        for d in range(len(data)):\n",
    "            predict = dt.predict(data[d][0])\n",
    "            prob = dt.predict_proba(data[d][0])\n",
    "            fraudscore = prob.transpose()[1]\n",
    "            fdr_each = calculateFDR(predict, fraudscore, data[d][1])\n",
    "            fdr_total[d] += fdr_each\n",
    "            fdr_list.append(fdr_each)\n",
    "        \n",
    "        fdr_df.append(fdr_list)\n",
    "    \n",
    "    avg_train = fdr_total[0] / iteration\n",
    "    avg_test = fdr_total[1] / iteration\n",
    "    avg_oot = fdr_total[2] / iteration\n",
    "    result = [avg_train, avg_test, avg_oot]\n",
    "\n",
    "    return fdr_df, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-04 22:57:48.076776\n",
      "Done -------------\t[20, 100, 10]\twith test fdr: 0.7261\t            2021-05-04 22:57:52\n",
      "Current maxparams:\t[20, 100, 10]\twith test fdr: 0.7261\n",
      "\n",
      "Done -------------\t[20, 100, 50]\twith test fdr: 0.7169\t            2021-05-04 22:57:57\n",
      "Current maxparams:\t[20, 100, 10]\twith test fdr: 0.7261\n",
      "\n",
      "Done -------------\t[20, 100, 100]\twith test fdr: 0.7060\t            2021-05-04 22:58:02\n",
      "Current maxparams:\t[20, 100, 10]\twith test fdr: 0.7261\n",
      "\n",
      "Done -------------\t[20, 200, 10]\twith test fdr: 0.7243\t            2021-05-04 22:58:08\n",
      "Current maxparams:\t[20, 100, 10]\twith test fdr: 0.7261\n",
      "\n",
      "Done -------------\t[20, 200, 50]\twith test fdr: 0.7317\t            2021-05-04 22:58:14\n",
      "Current maxparams:\t[20, 200, 50]\twith test fdr: 0.7317\n",
      "\n",
      "Done -------------\t[20, 200, 100]\twith test fdr: 0.7167\t            2021-05-04 22:58:21\n",
      "Current maxparams:\t[20, 200, 50]\twith test fdr: 0.7317\n",
      "\n",
      "Done -------------\t[25, 100, 10]\twith test fdr: 0.6966\t            2021-05-04 22:58:27\n",
      "Current maxparams:\t[20, 200, 50]\twith test fdr: 0.7317\n",
      "\n",
      "Done -------------\t[25, 100, 50]\twith test fdr: 0.7146\t            2021-05-04 22:58:34\n",
      "Current maxparams:\t[20, 200, 50]\twith test fdr: 0.7317\n",
      "\n",
      "Done -------------\t[25, 100, 100]\twith test fdr: 0.7086\t            2021-05-04 22:58:40\n",
      "Current maxparams:\t[20, 200, 50]\twith test fdr: 0.7317\n",
      "\n",
      "Done -------------\t[25, 200, 10]\twith test fdr: 0.7006\t            2021-05-04 22:58:47\n",
      "Current maxparams:\t[20, 200, 50]\twith test fdr: 0.7317\n",
      "\n",
      "Done -------------\t[25, 200, 50]\twith test fdr: 0.7035\t            2021-05-04 22:58:56\n",
      "Current maxparams:\t[20, 200, 50]\twith test fdr: 0.7317\n",
      "\n",
      "Done -------------\t[25, 200, 100]\twith test fdr: 0.7335\t            2021-05-04 22:59:03\n",
      "Current maxparams:\t[25, 200, 100]\twith test fdr: 0.7335\n",
      "\n",
      "Done -------------\t[30, 100, 10]\twith test fdr: 0.7263\t            2021-05-04 22:59:11\n",
      "Current maxparams:\t[25, 200, 100]\twith test fdr: 0.7335\n",
      "\n",
      "Done -------------\t[30, 100, 50]\twith test fdr: 0.7047\t            2021-05-04 22:59:17\n",
      "Current maxparams:\t[25, 200, 100]\twith test fdr: 0.7335\n",
      "\n",
      "Done -------------\t[30, 100, 100]\twith test fdr: 0.6998\t            2021-05-04 22:59:24\n",
      "Current maxparams:\t[25, 200, 100]\twith test fdr: 0.7335\n",
      "\n",
      "Done -------------\t[30, 200, 10]\twith test fdr: 0.7250\t            2021-05-04 22:59:32\n",
      "Current maxparams:\t[25, 200, 100]\twith test fdr: 0.7335\n",
      "\n",
      "Done -------------\t[30, 200, 50]\twith test fdr: 0.7253\t            2021-05-04 22:59:42\n",
      "Current maxparams:\t[25, 200, 100]\twith test fdr: 0.7335\n",
      "\n",
      "Done -------------\t[30, 200, 100]\twith test fdr: 0.7462\t            2021-05-04 22:59:51\n",
      "Current maxparams:\t[30, 200, 100]\twith test fdr: 0.7462\n",
      "\n",
      "2021-05-04 22:59:51.144938\n",
      "features:\t\t30\n",
      "nodes:\t\t\t200\n",
      "max_depth:\t\t100\n",
      "with average test fdr:\t0.7462\n",
      "Duration:  0:02:03.068162\n"
     ]
    }
   ],
   "source": [
    "### apply function here\n",
    "dt_start_time = datetime.now()\n",
    "print(dt_start_time)\n",
    "# Adjusting hyperparameters\n",
    "features = [20, 25, 30]\n",
    "nodes = [100, 200] \n",
    "depth = [10, 50, 100]\n",
    "\n",
    "maximum = 0\n",
    "params=[]\n",
    "dt_fdr_all=[]\n",
    "dt_fdr_mean=[]\n",
    "for i in features:\n",
    "    for j in nodes:\n",
    "        for k in depth:\n",
    "            temp1, temp2 = decisiontreeFDR(X, X_oot, y, y_oot, iteration=10, \\\n",
    "                                          max_features = i, \\\n",
    "                                          max_leaf_nodes = j, \\\n",
    "                                          max_depth = k)\n",
    "            params.append([i, j, k])\n",
    "            dt_fdr_mean.append(temp2)\n",
    "            dt_fdr_all.append(temp1)\n",
    "            if maximum < temp2[1]:\n",
    "                maximum=temp2[1]\n",
    "                maxfdr=temp2\n",
    "                maxparams=[i,j,k]\n",
    "            current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print(f'Done -------------\\t{[i,j,k]}\\twith test fdr: {temp2[1]:.4f}\\t\\\n",
    "            {current_time}\\nCurrent maxparams:\\t{maxparams}\\twith test fdr: {maximum:.4f}\\n')\n",
    "\n",
    "dt_end_time = datetime.now()\n",
    "print(dt_end_time)\n",
    "print(f'features:\\t\\t{maxparams[0]}\\n\\\n",
    "nodes:\\t\\t\\t{maxparams[1]}\\n\\\n",
    "max_depth:\\t\\t{maxparams[2]}\\n\\\n",
    "with average test fdr:\\t{maximum:.4f}')\n",
    "# Duration of the cell\n",
    "print('Duration: ', dt_end_time - dt_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>fdr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[20, 100, 10]</td>\n",
       "      <td>[0.8051554609587696, 0.7260532650277436, 0.334...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[20, 100, 50]</td>\n",
       "      <td>[0.787944165833594, 0.7168581300182534, 0.3033...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[20, 100, 100]</td>\n",
       "      <td>[0.7866371980335785, 0.7060362941168866, 0.343...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[20, 200, 10]</td>\n",
       "      <td>[0.8330718610000221, 0.724280259068913, 0.3139...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[20, 200, 50]</td>\n",
       "      <td>[0.8904766303841983, 0.7316923097339567, 0.218...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[20, 200, 100]</td>\n",
       "      <td>[0.8949917359812944, 0.7167190871506094, 0.242...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[25, 100, 10]</td>\n",
       "      <td>[0.7994474673407261, 0.6966196403677154, 0.327...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[25, 100, 50]</td>\n",
       "      <td>[0.7958780134118915, 0.714591874080768, 0.3446...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[25, 100, 100]</td>\n",
       "      <td>[0.780594671290175, 0.7085878234440564, 0.3184...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[25, 200, 10]</td>\n",
       "      <td>[0.8093879125297081, 0.70056116576525, 0.26256...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[25, 200, 50]</td>\n",
       "      <td>[0.8969408235191108, 0.7034528552666204, 0.263...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[25, 200, 100]</td>\n",
       "      <td>[0.8865626997790501, 0.7334716885199767, 0.273...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[30, 100, 10]</td>\n",
       "      <td>[0.8222829969591239, 0.726251639185941, 0.3139...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[30, 100, 50]</td>\n",
       "      <td>[0.803467539351735, 0.7047433206629973, 0.3575...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[30, 100, 100]</td>\n",
       "      <td>[0.8018652179496766, 0.699767208768278, 0.2547...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[30, 200, 10]</td>\n",
       "      <td>[0.8493454546234762, 0.7250347770888024, 0.292...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[30, 200, 50]</td>\n",
       "      <td>[0.9008265886384847, 0.7253411535710282, 0.234...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[30, 200, 100]</td>\n",
       "      <td>[0.9059371122529024, 0.7461999181076445, 0.245...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            params                                                fdr\n",
       "0    [20, 100, 10]  [0.8051554609587696, 0.7260532650277436, 0.334...\n",
       "1    [20, 100, 50]  [0.787944165833594, 0.7168581300182534, 0.3033...\n",
       "2   [20, 100, 100]  [0.7866371980335785, 0.7060362941168866, 0.343...\n",
       "3    [20, 200, 10]  [0.8330718610000221, 0.724280259068913, 0.3139...\n",
       "4    [20, 200, 50]  [0.8904766303841983, 0.7316923097339567, 0.218...\n",
       "5   [20, 200, 100]  [0.8949917359812944, 0.7167190871506094, 0.242...\n",
       "6    [25, 100, 10]  [0.7994474673407261, 0.6966196403677154, 0.327...\n",
       "7    [25, 100, 50]  [0.7958780134118915, 0.714591874080768, 0.3446...\n",
       "8   [25, 100, 100]  [0.780594671290175, 0.7085878234440564, 0.3184...\n",
       "9    [25, 200, 10]  [0.8093879125297081, 0.70056116576525, 0.26256...\n",
       "10   [25, 200, 50]  [0.8969408235191108, 0.7034528552666204, 0.263...\n",
       "11  [25, 200, 100]  [0.8865626997790501, 0.7334716885199767, 0.273...\n",
       "12   [30, 100, 10]  [0.8222829969591239, 0.726251639185941, 0.3139...\n",
       "13   [30, 100, 50]  [0.803467539351735, 0.7047433206629973, 0.3575...\n",
       "14  [30, 100, 100]  [0.8018652179496766, 0.699767208768278, 0.2547...\n",
       "15   [30, 200, 10]  [0.8493454546234762, 0.7250347770888024, 0.292...\n",
       "16   [30, 200, 50]  [0.9008265886384847, 0.7253411535710282, 0.234...\n",
       "17  [30, 200, 100]  [0.9059371122529024, 0.7461999181076445, 0.245..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_output30 = pd.DataFrame({'params':params, 'fdr':dt_fdr_mean})\n",
    "dt_output30_alliter = pd.DataFrame(dt_fdr_all[0],columns=[str(params[0])+': train', 'test', 'oot'])\n",
    "for i in range(1,len(dt_fdr_all)):\n",
    "    dt_output30_alliter = pd.concat([dt_output30_alliter, pd.DataFrame(dt_fdr_all[i],columns=[str(params[i])+': train', 'test', 'oot'])], axis=1)\n",
    "dt_output30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>[20, 100, 10]: train</th>\n",
       "      <th>test</th>\n",
       "      <th>oot</th>\n",
       "      <th>[20, 100, 50]: train</th>\n",
       "      <th>test</th>\n",
       "      <th>oot</th>\n",
       "      <th>[20, 100, 100]: train</th>\n",
       "      <th>test</th>\n",
       "      <th>oot</th>\n",
       "      <th>[20, 200, 10]: train</th>\n",
       "      <th>test</th>\n",
       "      <th>oot</th>\n",
       "      <th>[20, 200, 50]: train</th>\n",
       "      <th>test</th>\n",
       "      <th>oot</th>\n",
       "      <th>[20, 200, 100]: train</th>\n",
       "      <th>test</th>\n",
       "      <th>oot</th>\n",
       "      <th>[25, 100, 10]: train</th>\n",
       "      <th>test</th>\n",
       "      <th>oot</th>\n",
       "      <th>[25, 100, 50]: train</th>\n",
       "      <th>test</th>\n",
       "      <th>oot</th>\n",
       "      <th>[25, 100, 100]: train</th>\n",
       "      <th>test</th>\n",
       "      <th>oot</th>\n",
       "      <th>[25, 200, 10]: train</th>\n",
       "      <th>test</th>\n",
       "      <th>oot</th>\n",
       "      <th>[25, 200, 50]: train</th>\n",
       "      <th>test</th>\n",
       "      <th>oot</th>\n",
       "      <th>[25, 200, 100]: train</th>\n",
       "      <th>test</th>\n",
       "      <th>oot</th>\n",
       "      <th>[30, 100, 10]: train</th>\n",
       "      <th>test</th>\n",
       "      <th>oot</th>\n",
       "      <th>[30, 100, 50]: train</th>\n",
       "      <th>test</th>\n",
       "      <th>oot</th>\n",
       "      <th>[30, 100, 100]: train</th>\n",
       "      <th>test</th>\n",
       "      <th>oot</th>\n",
       "      <th>[30, 200, 10]: train</th>\n",
       "      <th>test</th>\n",
       "      <th>oot</th>\n",
       "      <th>[30, 200, 50]: train</th>\n",
       "      <th>test</th>\n",
       "      <th>oot</th>\n",
       "      <th>[30, 200, 100]: train</th>\n",
       "      <th>test</th>\n",
       "      <th>oot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.797806</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.536313</td>\n",
       "      <td>0.797256</td>\n",
       "      <td>0.726415</td>\n",
       "      <td>0.391061</td>\n",
       "      <td>0.774495</td>\n",
       "      <td>0.662222</td>\n",
       "      <td>0.268156</td>\n",
       "      <td>0.843798</td>\n",
       "      <td>0.697674</td>\n",
       "      <td>0.122905</td>\n",
       "      <td>0.882801</td>\n",
       "      <td>0.668246</td>\n",
       "      <td>0.217877</td>\n",
       "      <td>0.887122</td>\n",
       "      <td>0.740586</td>\n",
       "      <td>0.189944</td>\n",
       "      <td>0.790698</td>\n",
       "      <td>0.668161</td>\n",
       "      <td>0.290503</td>\n",
       "      <td>0.809375</td>\n",
       "      <td>0.714912</td>\n",
       "      <td>0.245810</td>\n",
       "      <td>0.792248</td>\n",
       "      <td>0.708520</td>\n",
       "      <td>0.290503</td>\n",
       "      <td>0.712963</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.296089</td>\n",
       "      <td>0.891850</td>\n",
       "      <td>0.747826</td>\n",
       "      <td>0.290503</td>\n",
       "      <td>0.895223</td>\n",
       "      <td>0.748858</td>\n",
       "      <td>0.251397</td>\n",
       "      <td>0.815913</td>\n",
       "      <td>0.713656</td>\n",
       "      <td>0.324022</td>\n",
       "      <td>0.811912</td>\n",
       "      <td>0.686957</td>\n",
       "      <td>0.379888</td>\n",
       "      <td>0.746565</td>\n",
       "      <td>0.671362</td>\n",
       "      <td>0.301676</td>\n",
       "      <td>0.879020</td>\n",
       "      <td>0.730233</td>\n",
       "      <td>0.273743</td>\n",
       "      <td>0.879630</td>\n",
       "      <td>0.740909</td>\n",
       "      <td>0.268156</td>\n",
       "      <td>0.900915</td>\n",
       "      <td>0.759434</td>\n",
       "      <td>0.279330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.808642</td>\n",
       "      <td>0.759091</td>\n",
       "      <td>0.167598</td>\n",
       "      <td>0.812883</td>\n",
       "      <td>0.731481</td>\n",
       "      <td>0.284916</td>\n",
       "      <td>0.828173</td>\n",
       "      <td>0.747748</td>\n",
       "      <td>0.279330</td>\n",
       "      <td>0.823622</td>\n",
       "      <td>0.759657</td>\n",
       "      <td>0.173184</td>\n",
       "      <td>0.877676</td>\n",
       "      <td>0.705607</td>\n",
       "      <td>0.245810</td>\n",
       "      <td>0.905054</td>\n",
       "      <td>0.725581</td>\n",
       "      <td>0.223464</td>\n",
       "      <td>0.787692</td>\n",
       "      <td>0.623853</td>\n",
       "      <td>0.385475</td>\n",
       "      <td>0.802839</td>\n",
       "      <td>0.705128</td>\n",
       "      <td>0.402235</td>\n",
       "      <td>0.777439</td>\n",
       "      <td>0.698113</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.827744</td>\n",
       "      <td>0.721698</td>\n",
       "      <td>0.363128</td>\n",
       "      <td>0.879815</td>\n",
       "      <td>0.694064</td>\n",
       "      <td>0.346369</td>\n",
       "      <td>0.897081</td>\n",
       "      <td>0.760369</td>\n",
       "      <td>0.178771</td>\n",
       "      <td>0.840361</td>\n",
       "      <td>0.754902</td>\n",
       "      <td>0.256983</td>\n",
       "      <td>0.805338</td>\n",
       "      <td>0.675325</td>\n",
       "      <td>0.519553</td>\n",
       "      <td>0.805296</td>\n",
       "      <td>0.743363</td>\n",
       "      <td>0.245810</td>\n",
       "      <td>0.837423</td>\n",
       "      <td>0.731481</td>\n",
       "      <td>0.279330</td>\n",
       "      <td>0.910632</td>\n",
       "      <td>0.748858</td>\n",
       "      <td>0.134078</td>\n",
       "      <td>0.892638</td>\n",
       "      <td>0.736111</td>\n",
       "      <td>0.290503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.803709</td>\n",
       "      <td>0.683258</td>\n",
       "      <td>0.273743</td>\n",
       "      <td>0.795349</td>\n",
       "      <td>0.686099</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.796610</td>\n",
       "      <td>0.684932</td>\n",
       "      <td>0.340782</td>\n",
       "      <td>0.819572</td>\n",
       "      <td>0.766355</td>\n",
       "      <td>0.430168</td>\n",
       "      <td>0.895366</td>\n",
       "      <td>0.708543</td>\n",
       "      <td>0.379888</td>\n",
       "      <td>0.877193</td>\n",
       "      <td>0.659751</td>\n",
       "      <td>0.162011</td>\n",
       "      <td>0.802731</td>\n",
       "      <td>0.660287</td>\n",
       "      <td>0.446927</td>\n",
       "      <td>0.779661</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.223464</td>\n",
       "      <td>0.806061</td>\n",
       "      <td>0.759615</td>\n",
       "      <td>0.318436</td>\n",
       "      <td>0.814063</td>\n",
       "      <td>0.692982</td>\n",
       "      <td>0.195531</td>\n",
       "      <td>0.884323</td>\n",
       "      <td>0.696682</td>\n",
       "      <td>0.290503</td>\n",
       "      <td>0.882535</td>\n",
       "      <td>0.723982</td>\n",
       "      <td>0.273743</td>\n",
       "      <td>0.787402</td>\n",
       "      <td>0.695279</td>\n",
       "      <td>0.368715</td>\n",
       "      <td>0.804805</td>\n",
       "      <td>0.722772</td>\n",
       "      <td>0.424581</td>\n",
       "      <td>0.803982</td>\n",
       "      <td>0.702326</td>\n",
       "      <td>0.217877</td>\n",
       "      <td>0.849145</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.268156</td>\n",
       "      <td>0.911628</td>\n",
       "      <td>0.690583</td>\n",
       "      <td>0.245810</td>\n",
       "      <td>0.903418</td>\n",
       "      <td>0.728205</td>\n",
       "      <td>0.273743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.808096</td>\n",
       "      <td>0.751244</td>\n",
       "      <td>0.346369</td>\n",
       "      <td>0.767188</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.329609</td>\n",
       "      <td>0.795527</td>\n",
       "      <td>0.739669</td>\n",
       "      <td>0.340782</td>\n",
       "      <td>0.809240</td>\n",
       "      <td>0.725888</td>\n",
       "      <td>0.407821</td>\n",
       "      <td>0.903077</td>\n",
       "      <td>0.784404</td>\n",
       "      <td>0.167598</td>\n",
       "      <td>0.891135</td>\n",
       "      <td>0.715556</td>\n",
       "      <td>0.391061</td>\n",
       "      <td>0.794671</td>\n",
       "      <td>0.669565</td>\n",
       "      <td>0.245810</td>\n",
       "      <td>0.813170</td>\n",
       "      <td>0.734884</td>\n",
       "      <td>0.486034</td>\n",
       "      <td>0.822458</td>\n",
       "      <td>0.722488</td>\n",
       "      <td>0.240223</td>\n",
       "      <td>0.781202</td>\n",
       "      <td>0.739726</td>\n",
       "      <td>0.106145</td>\n",
       "      <td>0.902022</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>0.167598</td>\n",
       "      <td>0.880775</td>\n",
       "      <td>0.695431</td>\n",
       "      <td>0.407821</td>\n",
       "      <td>0.827534</td>\n",
       "      <td>0.772947</td>\n",
       "      <td>0.413408</td>\n",
       "      <td>0.773273</td>\n",
       "      <td>0.688119</td>\n",
       "      <td>0.262570</td>\n",
       "      <td>0.788310</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>0.307263</td>\n",
       "      <td>0.871212</td>\n",
       "      <td>0.745192</td>\n",
       "      <td>0.379888</td>\n",
       "      <td>0.908810</td>\n",
       "      <td>0.742081</td>\n",
       "      <td>0.184358</td>\n",
       "      <td>0.910112</td>\n",
       "      <td>0.751020</td>\n",
       "      <td>0.217877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.821958</td>\n",
       "      <td>0.757732</td>\n",
       "      <td>0.329609</td>\n",
       "      <td>0.789634</td>\n",
       "      <td>0.712264</td>\n",
       "      <td>0.340782</td>\n",
       "      <td>0.777444</td>\n",
       "      <td>0.729064</td>\n",
       "      <td>0.402235</td>\n",
       "      <td>0.817323</td>\n",
       "      <td>0.695279</td>\n",
       "      <td>0.379888</td>\n",
       "      <td>0.883792</td>\n",
       "      <td>0.780374</td>\n",
       "      <td>0.173184</td>\n",
       "      <td>0.910377</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.256983</td>\n",
       "      <td>0.832787</td>\n",
       "      <td>0.724806</td>\n",
       "      <td>0.441341</td>\n",
       "      <td>0.821856</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.229050</td>\n",
       "      <td>0.771174</td>\n",
       "      <td>0.717949</td>\n",
       "      <td>0.217877</td>\n",
       "      <td>0.759398</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.223464</td>\n",
       "      <td>0.909667</td>\n",
       "      <td>0.700422</td>\n",
       "      <td>0.279330</td>\n",
       "      <td>0.875957</td>\n",
       "      <td>0.725581</td>\n",
       "      <td>0.346369</td>\n",
       "      <td>0.821918</td>\n",
       "      <td>0.763033</td>\n",
       "      <td>0.307263</td>\n",
       "      <td>0.811263</td>\n",
       "      <td>0.710900</td>\n",
       "      <td>0.508380</td>\n",
       "      <td>0.814757</td>\n",
       "      <td>0.679654</td>\n",
       "      <td>0.139665</td>\n",
       "      <td>0.857576</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.212291</td>\n",
       "      <td>0.902326</td>\n",
       "      <td>0.726457</td>\n",
       "      <td>0.251397</td>\n",
       "      <td>0.908100</td>\n",
       "      <td>0.738938</td>\n",
       "      <td>0.184358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.807988</td>\n",
       "      <td>0.728111</td>\n",
       "      <td>0.346369</td>\n",
       "      <td>0.796012</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.178771</td>\n",
       "      <td>0.723975</td>\n",
       "      <td>0.658120</td>\n",
       "      <td>0.402235</td>\n",
       "      <td>0.821538</td>\n",
       "      <td>0.706422</td>\n",
       "      <td>0.256983</td>\n",
       "      <td>0.906061</td>\n",
       "      <td>0.716346</td>\n",
       "      <td>0.212291</td>\n",
       "      <td>0.896445</td>\n",
       "      <td>0.782805</td>\n",
       "      <td>0.340782</td>\n",
       "      <td>0.740973</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.240223</td>\n",
       "      <td>0.780338</td>\n",
       "      <td>0.686636</td>\n",
       "      <td>0.284916</td>\n",
       "      <td>0.775542</td>\n",
       "      <td>0.716216</td>\n",
       "      <td>0.284916</td>\n",
       "      <td>0.823890</td>\n",
       "      <td>0.730233</td>\n",
       "      <td>0.307263</td>\n",
       "      <td>0.896445</td>\n",
       "      <td>0.737557</td>\n",
       "      <td>0.201117</td>\n",
       "      <td>0.893778</td>\n",
       "      <td>0.732057</td>\n",
       "      <td>0.284916</td>\n",
       "      <td>0.826421</td>\n",
       "      <td>0.677419</td>\n",
       "      <td>0.424581</td>\n",
       "      <td>0.808709</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.413408</td>\n",
       "      <td>0.813636</td>\n",
       "      <td>0.788462</td>\n",
       "      <td>0.178771</td>\n",
       "      <td>0.843373</td>\n",
       "      <td>0.730392</td>\n",
       "      <td>0.307263</td>\n",
       "      <td>0.898773</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.363128</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.769585</td>\n",
       "      <td>0.212291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.807927</td>\n",
       "      <td>0.716981</td>\n",
       "      <td>0.363128</td>\n",
       "      <td>0.804075</td>\n",
       "      <td>0.726087</td>\n",
       "      <td>0.402235</td>\n",
       "      <td>0.779970</td>\n",
       "      <td>0.732057</td>\n",
       "      <td>0.296089</td>\n",
       "      <td>0.846737</td>\n",
       "      <td>0.703349</td>\n",
       "      <td>0.284916</td>\n",
       "      <td>0.899529</td>\n",
       "      <td>0.744589</td>\n",
       "      <td>0.145251</td>\n",
       "      <td>0.893939</td>\n",
       "      <td>0.725962</td>\n",
       "      <td>0.351955</td>\n",
       "      <td>0.809380</td>\n",
       "      <td>0.748792</td>\n",
       "      <td>0.368715</td>\n",
       "      <td>0.799378</td>\n",
       "      <td>0.746667</td>\n",
       "      <td>0.324022</td>\n",
       "      <td>0.782071</td>\n",
       "      <td>0.755656</td>\n",
       "      <td>0.385475</td>\n",
       "      <td>0.871272</td>\n",
       "      <td>0.740260</td>\n",
       "      <td>0.217877</td>\n",
       "      <td>0.907576</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.256983</td>\n",
       "      <td>0.877273</td>\n",
       "      <td>0.783654</td>\n",
       "      <td>0.290503</td>\n",
       "      <td>0.815988</td>\n",
       "      <td>0.697561</td>\n",
       "      <td>0.234637</td>\n",
       "      <td>0.827893</td>\n",
       "      <td>0.762887</td>\n",
       "      <td>0.240223</td>\n",
       "      <td>0.825348</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.312849</td>\n",
       "      <td>0.862285</td>\n",
       "      <td>0.681223</td>\n",
       "      <td>0.368715</td>\n",
       "      <td>0.883969</td>\n",
       "      <td>0.685446</td>\n",
       "      <td>0.122905</td>\n",
       "      <td>0.906442</td>\n",
       "      <td>0.763889</td>\n",
       "      <td>0.290503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.812793</td>\n",
       "      <td>0.709251</td>\n",
       "      <td>0.290503</td>\n",
       "      <td>0.781955</td>\n",
       "      <td>0.743842</td>\n",
       "      <td>0.156425</td>\n",
       "      <td>0.785824</td>\n",
       "      <td>0.698630</td>\n",
       "      <td>0.469274</td>\n",
       "      <td>0.864275</td>\n",
       "      <td>0.713656</td>\n",
       "      <td>0.296089</td>\n",
       "      <td>0.866864</td>\n",
       "      <td>0.776042</td>\n",
       "      <td>0.256983</td>\n",
       "      <td>0.900901</td>\n",
       "      <td>0.683168</td>\n",
       "      <td>0.134078</td>\n",
       "      <td>0.803571</td>\n",
       "      <td>0.719388</td>\n",
       "      <td>0.312849</td>\n",
       "      <td>0.796102</td>\n",
       "      <td>0.741294</td>\n",
       "      <td>0.256983</td>\n",
       "      <td>0.769841</td>\n",
       "      <td>0.684874</td>\n",
       "      <td>0.256983</td>\n",
       "      <td>0.830015</td>\n",
       "      <td>0.730233</td>\n",
       "      <td>0.357542</td>\n",
       "      <td>0.898638</td>\n",
       "      <td>0.676329</td>\n",
       "      <td>0.156425</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.734783</td>\n",
       "      <td>0.251397</td>\n",
       "      <td>0.833837</td>\n",
       "      <td>0.762136</td>\n",
       "      <td>0.173184</td>\n",
       "      <td>0.764431</td>\n",
       "      <td>0.687225</td>\n",
       "      <td>0.363128</td>\n",
       "      <td>0.803709</td>\n",
       "      <td>0.683258</td>\n",
       "      <td>0.139665</td>\n",
       "      <td>0.869767</td>\n",
       "      <td>0.708520</td>\n",
       "      <td>0.284916</td>\n",
       "      <td>0.894009</td>\n",
       "      <td>0.709677</td>\n",
       "      <td>0.318436</td>\n",
       "      <td>0.910906</td>\n",
       "      <td>0.755760</td>\n",
       "      <td>0.245810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.757164</td>\n",
       "      <td>0.726829</td>\n",
       "      <td>0.229050</td>\n",
       "      <td>0.794349</td>\n",
       "      <td>0.722944</td>\n",
       "      <td>0.251397</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.296089</td>\n",
       "      <td>0.857988</td>\n",
       "      <td>0.744792</td>\n",
       "      <td>0.424581</td>\n",
       "      <td>0.891768</td>\n",
       "      <td>0.707547</td>\n",
       "      <td>0.150838</td>\n",
       "      <td>0.899371</td>\n",
       "      <td>0.659483</td>\n",
       "      <td>0.184358</td>\n",
       "      <td>0.781202</td>\n",
       "      <td>0.721461</td>\n",
       "      <td>0.178771</td>\n",
       "      <td>0.767939</td>\n",
       "      <td>0.685446</td>\n",
       "      <td>0.435754</td>\n",
       "      <td>0.747352</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.385475</td>\n",
       "      <td>0.792570</td>\n",
       "      <td>0.671171</td>\n",
       "      <td>0.251397</td>\n",
       "      <td>0.899536</td>\n",
       "      <td>0.696833</td>\n",
       "      <td>0.357542</td>\n",
       "      <td>0.903382</td>\n",
       "      <td>0.696356</td>\n",
       "      <td>0.156425</td>\n",
       "      <td>0.826748</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.346369</td>\n",
       "      <td>0.809006</td>\n",
       "      <td>0.683036</td>\n",
       "      <td>0.162011</td>\n",
       "      <td>0.800940</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.782132</td>\n",
       "      <td>0.713043</td>\n",
       "      <td>0.307263</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>0.756477</td>\n",
       "      <td>0.279330</td>\n",
       "      <td>0.912577</td>\n",
       "      <td>0.717593</td>\n",
       "      <td>0.240223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.825472</td>\n",
       "      <td>0.775862</td>\n",
       "      <td>0.458101</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.668182</td>\n",
       "      <td>0.324022</td>\n",
       "      <td>0.798799</td>\n",
       "      <td>0.707921</td>\n",
       "      <td>0.335196</td>\n",
       "      <td>0.826625</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.363128</td>\n",
       "      <td>0.897833</td>\n",
       "      <td>0.725225</td>\n",
       "      <td>0.234637</td>\n",
       "      <td>0.888379</td>\n",
       "      <td>0.724299</td>\n",
       "      <td>0.189944</td>\n",
       "      <td>0.850769</td>\n",
       "      <td>0.715596</td>\n",
       "      <td>0.363128</td>\n",
       "      <td>0.788122</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.558659</td>\n",
       "      <td>0.761760</td>\n",
       "      <td>0.626794</td>\n",
       "      <td>0.430168</td>\n",
       "      <td>0.880763</td>\n",
       "      <td>0.698745</td>\n",
       "      <td>0.307263</td>\n",
       "      <td>0.899536</td>\n",
       "      <td>0.665158</td>\n",
       "      <td>0.290503</td>\n",
       "      <td>0.897554</td>\n",
       "      <td>0.733645</td>\n",
       "      <td>0.290503</td>\n",
       "      <td>0.826709</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.290503</td>\n",
       "      <td>0.818045</td>\n",
       "      <td>0.763547</td>\n",
       "      <td>0.301676</td>\n",
       "      <td>0.816109</td>\n",
       "      <td>0.704762</td>\n",
       "      <td>0.329609</td>\n",
       "      <td>0.841521</td>\n",
       "      <td>0.759494</td>\n",
       "      <td>0.240223</td>\n",
       "      <td>0.907378</td>\n",
       "      <td>0.744589</td>\n",
       "      <td>0.178771</td>\n",
       "      <td>0.909502</td>\n",
       "      <td>0.741463</td>\n",
       "      <td>0.223464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   [20, 100, 10]: train      test       oot  [20, 100, 50]: train      test  \\\n",
       "0              0.797806  0.652174  0.536313              0.797256  0.726415   \n",
       "1              0.808642  0.759091  0.167598              0.812883  0.731481   \n",
       "2              0.803709  0.683258  0.273743              0.795349  0.686099   \n",
       "3              0.808096  0.751244  0.346369              0.767188  0.710526   \n",
       "4              0.821958  0.757732  0.329609              0.789634  0.712264   \n",
       "5              0.807988  0.728111  0.346369              0.796012  0.740741   \n",
       "6              0.807927  0.716981  0.363128              0.804075  0.726087   \n",
       "7              0.812793  0.709251  0.290503              0.781955  0.743842   \n",
       "8              0.757164  0.726829  0.229050              0.794349  0.722944   \n",
       "9              0.825472  0.775862  0.458101              0.740741  0.668182   \n",
       "\n",
       "        oot  [20, 100, 100]: train      test       oot  [20, 200, 10]: train  \\\n",
       "0  0.391061               0.774495  0.662222  0.268156              0.843798   \n",
       "1  0.284916               0.828173  0.747748  0.279330              0.823622   \n",
       "2  0.374302               0.796610  0.684932  0.340782              0.819572   \n",
       "3  0.329609               0.795527  0.739669  0.340782              0.809240   \n",
       "4  0.340782               0.777444  0.729064  0.402235              0.817323   \n",
       "5  0.178771               0.723975  0.658120  0.402235              0.821538   \n",
       "6  0.402235               0.779970  0.732057  0.296089              0.846737   \n",
       "7  0.156425               0.785824  0.698630  0.469274              0.864275   \n",
       "8  0.251397               0.805556  0.700000  0.296089              0.857988   \n",
       "9  0.324022               0.798799  0.707921  0.335196              0.826625   \n",
       "\n",
       "       test       oot  [20, 200, 50]: train      test       oot  \\\n",
       "0  0.697674  0.122905              0.882801  0.668246  0.217877   \n",
       "1  0.759657  0.173184              0.877676  0.705607  0.245810   \n",
       "2  0.766355  0.430168              0.895366  0.708543  0.379888   \n",
       "3  0.725888  0.407821              0.903077  0.784404  0.167598   \n",
       "4  0.695279  0.379888              0.883792  0.780374  0.173184   \n",
       "5  0.706422  0.256983              0.906061  0.716346  0.212291   \n",
       "6  0.703349  0.284916              0.899529  0.744589  0.145251   \n",
       "7  0.713656  0.296089              0.866864  0.776042  0.256983   \n",
       "8  0.744792  0.424581              0.891768  0.707547  0.150838   \n",
       "9  0.729730  0.363128              0.897833  0.725225  0.234637   \n",
       "\n",
       "   [20, 200, 100]: train      test       oot  [25, 100, 10]: train      test  \\\n",
       "0               0.887122  0.740586  0.189944              0.790698  0.668161   \n",
       "1               0.905054  0.725581  0.223464              0.787692  0.623853   \n",
       "2               0.877193  0.659751  0.162011              0.802731  0.660287   \n",
       "3               0.891135  0.715556  0.391061              0.794671  0.669565   \n",
       "4               0.910377  0.750000  0.256983              0.832787  0.724806   \n",
       "5               0.896445  0.782805  0.340782              0.740973  0.714286   \n",
       "6               0.893939  0.725962  0.351955              0.809380  0.748792   \n",
       "7               0.900901  0.683168  0.134078              0.803571  0.719388   \n",
       "8               0.899371  0.659483  0.184358              0.781202  0.721461   \n",
       "9               0.888379  0.724299  0.189944              0.850769  0.715596   \n",
       "\n",
       "        oot  [25, 100, 50]: train      test       oot  [25, 100, 100]: train  \\\n",
       "0  0.290503              0.809375  0.714912  0.245810               0.792248   \n",
       "1  0.385475              0.802839  0.705128  0.402235               0.777439   \n",
       "2  0.446927              0.779661  0.666667  0.223464               0.806061   \n",
       "3  0.245810              0.813170  0.734884  0.486034               0.822458   \n",
       "4  0.441341              0.821856  0.750000  0.229050               0.771174   \n",
       "5  0.240223              0.780338  0.686636  0.284916               0.775542   \n",
       "6  0.368715              0.799378  0.746667  0.324022               0.782071   \n",
       "7  0.312849              0.796102  0.741294  0.256983               0.769841   \n",
       "8  0.178771              0.767939  0.685446  0.435754               0.747352   \n",
       "9  0.363128              0.788122  0.714286  0.558659               0.761760   \n",
       "\n",
       "       test       oot  [25, 200, 10]: train      test       oot  \\\n",
       "0  0.708520  0.290503              0.712963  0.590909  0.296089   \n",
       "1  0.698113  0.374302              0.827744  0.721698  0.363128   \n",
       "2  0.759615  0.318436              0.814063  0.692982  0.195531   \n",
       "3  0.722488  0.240223              0.781202  0.739726  0.106145   \n",
       "4  0.717949  0.217877              0.759398  0.689655  0.223464   \n",
       "5  0.716216  0.284916              0.823890  0.730233  0.307263   \n",
       "6  0.755656  0.385475              0.871272  0.740260  0.217877   \n",
       "7  0.684874  0.256983              0.830015  0.730233  0.357542   \n",
       "8  0.695652  0.385475              0.792570  0.671171  0.251397   \n",
       "9  0.626794  0.430168              0.880763  0.698745  0.307263   \n",
       "\n",
       "   [25, 200, 50]: train      test       oot  [25, 200, 100]: train      test  \\\n",
       "0              0.891850  0.747826  0.290503               0.895223  0.748858   \n",
       "1              0.879815  0.694064  0.346369               0.897081  0.760369   \n",
       "2              0.884323  0.696682  0.290503               0.882535  0.723982   \n",
       "3              0.902022  0.688889  0.167598               0.880775  0.695431   \n",
       "4              0.909667  0.700422  0.279330               0.875957  0.725581   \n",
       "5              0.896445  0.737557  0.201117               0.893778  0.732057   \n",
       "6              0.907576  0.730769  0.256983               0.877273  0.783654   \n",
       "7              0.898638  0.676329  0.156425               0.862069  0.734783   \n",
       "8              0.899536  0.696833  0.357542               0.903382  0.696356   \n",
       "9              0.899536  0.665158  0.290503               0.897554  0.733645   \n",
       "\n",
       "        oot  [30, 100, 10]: train      test       oot  [30, 100, 50]: train  \\\n",
       "0  0.251397              0.815913  0.713656  0.324022              0.811912   \n",
       "1  0.178771              0.840361  0.754902  0.256983              0.805338   \n",
       "2  0.273743              0.787402  0.695279  0.368715              0.804805   \n",
       "3  0.407821              0.827534  0.772947  0.413408              0.773273   \n",
       "4  0.346369              0.821918  0.763033  0.307263              0.811263   \n",
       "5  0.284916              0.826421  0.677419  0.424581              0.808709   \n",
       "6  0.290503              0.815988  0.697561  0.234637              0.827893   \n",
       "7  0.251397              0.833837  0.762136  0.173184              0.764431   \n",
       "8  0.156425              0.826748  0.714286  0.346369              0.809006   \n",
       "9  0.290503              0.826709  0.711297  0.290503              0.818045   \n",
       "\n",
       "       test       oot  [30, 100, 100]: train      test       oot  \\\n",
       "0  0.686957  0.379888               0.746565  0.671362  0.301676   \n",
       "1  0.675325  0.519553               0.805296  0.743363  0.245810   \n",
       "2  0.722772  0.424581               0.803982  0.702326  0.217877   \n",
       "3  0.688119  0.262570               0.788310  0.638298  0.307263   \n",
       "4  0.710900  0.508380               0.814757  0.679654  0.139665   \n",
       "5  0.666667  0.413408               0.813636  0.788462  0.178771   \n",
       "6  0.762887  0.240223               0.825348  0.647059  0.312849   \n",
       "7  0.687225  0.363128               0.803709  0.683258  0.139665   \n",
       "8  0.683036  0.162011               0.800940  0.739130  0.374302   \n",
       "9  0.763547  0.301676               0.816109  0.704762  0.329609   \n",
       "\n",
       "   [30, 200, 10]: train      test       oot  [30, 200, 50]: train      test  \\\n",
       "0              0.879020  0.730233  0.273743              0.879630  0.740909   \n",
       "1              0.837423  0.731481  0.279330              0.910632  0.748858   \n",
       "2              0.849145  0.720000  0.268156              0.911628  0.690583   \n",
       "3              0.871212  0.745192  0.379888              0.908810  0.742081   \n",
       "4              0.857576  0.730769  0.212291              0.902326  0.726457   \n",
       "5              0.843373  0.730392  0.307263              0.898773  0.708333   \n",
       "6              0.862285  0.681223  0.368715              0.883969  0.685446   \n",
       "7              0.869767  0.708520  0.284916              0.894009  0.709677   \n",
       "8              0.782132  0.713043  0.307263              0.911111  0.756477   \n",
       "9              0.841521  0.759494  0.240223              0.907378  0.744589   \n",
       "\n",
       "        oot  [30, 200, 100]: train      test       oot  \n",
       "0  0.268156               0.900915  0.759434  0.279330  \n",
       "1  0.134078               0.892638  0.736111  0.290503  \n",
       "2  0.245810               0.903418  0.728205  0.273743  \n",
       "3  0.184358               0.910112  0.751020  0.217877  \n",
       "4  0.251397               0.908100  0.738938  0.184358  \n",
       "5  0.363128               0.904762  0.769585  0.212291  \n",
       "6  0.122905               0.906442  0.763889  0.290503  \n",
       "7  0.318436               0.910906  0.755760  0.245810  \n",
       "8  0.279330               0.912577  0.717593  0.240223  \n",
       "9  0.178771               0.909502  0.741463  0.223464  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_output30_alliter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_output30.to_csv('results/dt_30vars_mean.csv')\n",
    "dt_output30_alliter.to_csv('results/dt_30vars_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of Features</th>\n",
       "      <th>blank1</th>\n",
       "      <th>Max Nodes</th>\n",
       "      <th>Max Depth</th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "      <th>OOT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.805155</td>\n",
       "      <td>0.726053</td>\n",
       "      <td>0.334078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>0.787944</td>\n",
       "      <td>0.716858</td>\n",
       "      <td>0.303352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>0.786637</td>\n",
       "      <td>0.706036</td>\n",
       "      <td>0.343017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "      <td>0.833072</td>\n",
       "      <td>0.724280</td>\n",
       "      <td>0.313966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200</td>\n",
       "      <td>50</td>\n",
       "      <td>0.890477</td>\n",
       "      <td>0.731692</td>\n",
       "      <td>0.218436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200</td>\n",
       "      <td>100</td>\n",
       "      <td>0.894992</td>\n",
       "      <td>0.716719</td>\n",
       "      <td>0.242458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.799447</td>\n",
       "      <td>0.696620</td>\n",
       "      <td>0.327374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>0.795878</td>\n",
       "      <td>0.714592</td>\n",
       "      <td>0.344693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>0.780595</td>\n",
       "      <td>0.708588</td>\n",
       "      <td>0.318436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "      <td>0.809388</td>\n",
       "      <td>0.700561</td>\n",
       "      <td>0.262570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200</td>\n",
       "      <td>50</td>\n",
       "      <td>0.896941</td>\n",
       "      <td>0.703453</td>\n",
       "      <td>0.263687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200</td>\n",
       "      <td>100</td>\n",
       "      <td>0.886563</td>\n",
       "      <td>0.733472</td>\n",
       "      <td>0.273184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.822283</td>\n",
       "      <td>0.726252</td>\n",
       "      <td>0.313966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>0.803468</td>\n",
       "      <td>0.704743</td>\n",
       "      <td>0.357542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>0.801865</td>\n",
       "      <td>0.699767</td>\n",
       "      <td>0.254749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "      <td>0.849345</td>\n",
       "      <td>0.725035</td>\n",
       "      <td>0.292179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200</td>\n",
       "      <td>50</td>\n",
       "      <td>0.900827</td>\n",
       "      <td>0.725341</td>\n",
       "      <td>0.234637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200</td>\n",
       "      <td>100</td>\n",
       "      <td>0.905937</td>\n",
       "      <td>0.746200</td>\n",
       "      <td>0.245810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Number of Features  blank1  Max Nodes  Max Depth     Train      Test  \\\n",
       "0                   20     NaN        100         10  0.805155  0.726053   \n",
       "1                   20     NaN        100         50  0.787944  0.716858   \n",
       "2                   20     NaN        100        100  0.786637  0.706036   \n",
       "3                   20     NaN        200         10  0.833072  0.724280   \n",
       "4                   20     NaN        200         50  0.890477  0.731692   \n",
       "5                   20     NaN        200        100  0.894992  0.716719   \n",
       "6                   25     NaN        100         10  0.799447  0.696620   \n",
       "7                   25     NaN        100         50  0.795878  0.714592   \n",
       "8                   25     NaN        100        100  0.780595  0.708588   \n",
       "9                   25     NaN        200         10  0.809388  0.700561   \n",
       "10                  25     NaN        200         50  0.896941  0.703453   \n",
       "11                  25     NaN        200        100  0.886563  0.733472   \n",
       "12                  30     NaN        100         10  0.822283  0.726252   \n",
       "13                  30     NaN        100         50  0.803468  0.704743   \n",
       "14                  30     NaN        100        100  0.801865  0.699767   \n",
       "15                  30     NaN        200         10  0.849345  0.725035   \n",
       "16                  30     NaN        200         50  0.900827  0.725341   \n",
       "17                  30     NaN        200        100  0.905937  0.746200   \n",
       "\n",
       "         OOT  \n",
       "0   0.334078  \n",
       "1   0.303352  \n",
       "2   0.343017  \n",
       "3   0.313966  \n",
       "4   0.218436  \n",
       "5   0.242458  \n",
       "6   0.327374  \n",
       "7   0.344693  \n",
       "8   0.318436  \n",
       "9   0.262570  \n",
       "10  0.263687  \n",
       "11  0.273184  \n",
       "12  0.313966  \n",
       "13  0.357542  \n",
       "14  0.254749  \n",
       "15  0.292179  \n",
       "16  0.234637  \n",
       "17  0.245810  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating Summary Table\n",
    "dt_summary = dt_output30.copy()\n",
    "dt_summary['Number of Features'] = dt_summary['params'].apply(lambda x: x[0])\n",
    "dt_summary['blank1'] = np.nan\n",
    "dt_summary['Max Nodes'] = dt_summary['params'].apply(lambda x: x[1])\n",
    "dt_summary['Max Depth'] = dt_summary['params'].apply(lambda x: x[2])\n",
    "\n",
    "dt_summary['Train'] = dt_summary['fdr'].apply(lambda x: x[0])\n",
    "dt_summary['Test'] = dt_summary['fdr'].apply(lambda x: x[1])\n",
    "dt_summary['OOT'] = dt_summary['fdr'].apply(lambda x: x[2])\n",
    "dt_summary.drop(columns = ['params', 'fdr'], inplace = True)\n",
    "dt_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_summary.to_excel('Results/dt_summary.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
